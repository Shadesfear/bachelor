\documentclass[12pt]{report}
\usepackage{pck-english}
%\usepackage{algorithmicx}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{standalone}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{11}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
%numbers=left,
%stepnumber = 1,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            %
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}



\begin{document}

\subfile{forside.tex}

\section*{Abstract}
\label{sec:label}


\newpage
\section*{Acknowledgement}
\label{sec:ackow}

\newpage

\tableofcontents

\newpage
\chapter{introduction}
\label{sec:intro}
\section{Background}
\label{subsec:label}
\section{Goal}
\label{subsec:label}






\chapter{Theory}
\label{sec:label}

\section{K-Means}
\label{subsec:kmeans}

K-means clustering is a method developed  of partitioning a set of points $X = \{x_1, x_2, ..., x_n\}$ in a d-dimensional space into a set of clusters $S = \{s_1, s_2, ..., s_k\}$, hence the name K-means. The algorithm takes n points and k number of clusters, it returns points split up into these clusters where an assignment has been made for each point to a given cluster. This problem is what is called a NP-hard problem this means that it's computationally hard. KMeans usually is one of the fastest clustering methods, but it suffers that it might converge to a local minimum instead of the sought after global minimum, but there are ways to more or less correct this fault.\cite{lloyd}

\section{Lloyd's Algorithm}
\label{subsec:lloyds}

The most common implementation of k-means is the so called Lloyd's Algorithm that uses an iterative refinement approach. To start of some initial centroids as the center of each cluster is called, is chosen at random, then at each iteration of the algorithm two steps are executed, and repeated until the individual clusters does not change anymore (This is called convergence), or is terminated by an upper bound of iterations. There are many methods for choosing the initial centroids, a common method is the Forgy method, where k random points of the input data set is chosen as the centroid. Another example is the KMeans++ method of initializing centroids, that aims to by careful seeding have a better start for the algorithm, and such reach convergence faster.\cite{plusplus} The two steps for each iteration can be outlined as such: \\

\textbf{Assignment:} For each data point, compute all distances to all centroids, and find the nearest one. assign this point to the cluster.\\
\textbf{Update:} For each cluster calculate the mean of all the assigned points to the cluster and move the cluster to the mean\\ \\

In a more strict notation it's as follow:

\begin{algorithm}[h!]
  \caption{Kmeans algorithm}
  \begin{algorithmic}

    \Procedure{Kmeans}{}
    \State $\textit{X} \gets \{x_1, x_2, .. x_n\}$
    \State $c_1, ..c_2,...,c_k \gets$ RANDOM(points, K)
    \While{not converged}
    \ForAll{ $x \in X$ }
    find $\phi_c(x)$ (closest centroids to x)
    \EndFor
    \ForAll{$c \in C$}
    $c =$ mean($\{x \in X | \phi_C(x) = c\}$)
    \EndFor
    \EndWhile
    \EndProcedure
  \end{algorithmic}

\end{algorithm}

The algorithm has some problems, it has to compute the distance from each point to each cluster in each iteration, this takes computational power. To compute the distance between two points $p,q \in \mathbb{R}^d$ we have to do the following computation:

\begin{equation}
  L = ||p - q||^2 = (p_1 - q_1)^2 +...+(p_d - q_d)^2
\end{equation}
This is known as the euclidian distance, this can be written more compactly as

\begin{equation}
  L = \sqrt{\sum_{i=1}^d(p_i - q_i)^2}
\end{equation}
This gives d subtractions, d multiplications and d-1 sums, and then for comparison with the previous cluster calculate the following: $d_{min} > d$ \\
This evaluates to $3\cdot d$ calculations for each point and cluster. Thus for each iteration we would need to execute $3d \cdot n \cdot k$ operations.

\section{Convergence}
\label{subsec:Convergens}



\section{KMeans++}
When we initialize the clusters randomly, we don't know how well the clusters represent the optimal clusters that we are trying to find in the end, this may in some cases lead to some  bad centroids that takes a very long time to converge to til global minimum. Instead we want to use another method for initializing the centroids. For this purpose the kmeans++ algorithm was developed in 2007 by David Arthur and Sergei Vassilvitskii \cite{plusplus}.
First we let $D(x)$ denote the shortest (euclidean) distance from a data point $x$ to the closest centroids that we have already chosen. And we also define a probability as the following:
\begin{equation}
P(x) = \frac{D(x)^2}{\sum_{x\in X}D(x)^2}
\end{equation}
Then the algorithm is explained as such:
\begin{itemize}
  \item Take one center $c_1$ chosen at random from the set of points $X$
  \item Take a new center $c_i$ choosing $x\in X$ with probability $P(x)$.
  \item Repeat until we have k centers.
  \item Continue with normal KMeans
\end{itemize}

Here I have just pasted some of the original experimental results that David and Sergei found in their original paper.

\begin{table}[h!]
  \centering
    \begin{tabular}{|c| c |c| c| }
      & Average $\phi$ & Minimum $\phi$ & Average time (Seconds) \\
      k & Random KMeans++ & Random KMeans++ & Random KMeans++ \\
      \hline
      2&3&  ell8 & cell9
    \end{tabular}
    \caption{Table of results from original KMeans++ Paper.}
    \label{table:kpp}
\end{table}

The faster times the results show comes from the fact that with proper initialization of the centroids we reach convergence faster than with random initialized centroids. As discussed in \ref{subsec:lloyds} each iteration of the algorithm will need $3d \cdot n\cdot k$ operations, so decreasing the number of iterations will have a huge impact on performance.

\section{Numpy}
\label{sec:numpy}
NumPy is a math library for python, that adds support for large matrix operations in any dimension, with support for higher level operations. NumPy adds support for array programming that we will utilize greatly in this project, this also means that we will have to write less code and use easier syntax that lets the programmer focus on creating software and not understating complex syntax.
\subsection{Broadcasting}
\label{subsec:Broadcasting}

On of the more advanced features of NumPy that we utilize in this project is broadcasting, broadcasting is NumPy's way of adding, subtracting or any other arithmetic on arrays of different sizes. Basically NumPy duplicates the smaller array such that the size and dimension of the smaller array matches the bigger array. A basic example of how this is handled can be seen in figure \ref{fig:broadcasting}.

\begin{figure}[H]
  \centering
  \includegraphics[width = 0.6\textwidth]{images/broadcasting.png}
  \caption{An example of how NumPy broadcasting works.\label{fig:broadcasting}}
\end{figure}





\section{Bohrium}
\label{subsec:Bohrium}
Bohrium is framework that aims to present a way to speed-up array programming, it's a modular framework with support for multiple alternate front end and back ends. Bohrium was created in 2013, by Mads R. B. Kristensen

\subsection{How it works}
\label{subsec:hiw}
Bohrium lazily record any array operations that is used, this could as an example be from NumPy, and turn them into bytecode instruction set. An overview can be seen in figure \ref{fig:bohrium}

\begin{figure}[H]
  \centering
  \label{fig:bohrium}
  \input{bohrium_overview}
\caption{The components that Bohrium are made out of}
\end{figure}



\section{Vectorization}
\label{subsec:vectorization}

The point of vectorization is to generalize operations on scalars to apply on vectors, matrices and also higher-dimensional arrays. The idea is to do operations on an entire set of values instead of each single item in the set. As en example the operation of adding two arrays together in a scalar function would look like this implemented in python
\begin{lstlisting}[language=C]
  for (i=0; i < n; i++)
      for (j = 0; j < n; j++)
          a[index_a][index_b] += b[index_a][index_b]
\end{lstlisting}

This tedious way of coding can now be abstracted away, as more and more programming languages and libraries support what is known as array programming, as an example in the library for python called Numpy the code for adding to vectors together can the trivialized to $a + b = c$ this leads to simpler code and it makes it possible for the programmer to speak the same language as mathematicians.

Vectorized operations are typically allows for the operations to be run in parallel, thus speeding up the operation time. This in turn allows us further to run our code on GPU's which should lead to even greater speedups. These speedups will be discussed en greater detail when we discuss my vectorized implementation of the KMeans algorithm and my results.

\section{OpenMP}
\label{subsec:openmp}
OpenMP is  library that supports what is known as shared memory multiproccesing. So when programming inside the framework of OpenMP all of the threads utilize the same memory and data. With OpenMP it's easy to paralellize code on the CPU, just by adding a pragma to the code. An example of this could to automatically try to paralellize a for loop.




\section{OpenCL}
\label{subsec:opencl}




\chapter{Implementation}
\label{subsec:implement}
\section{Overview}
\label{subsec:overview}
The program that was developed is aptly named Bohrium Kmeans, and as discussed is developed in Python using the Bohrium library for automatic GPU acceleration created by Mads. R. B. Kristensen. Bohrium Kmeans as most other kmeans implementations takes a set of points in any number of dimensions and number of clusters, this means that there is a large overlap in the way to use Bohrium Kmeans compared to some of the more popular libraries such as SKlearn for Python.\\
Bohrium Kmeans allows for alot of different parameters to be changed, as a concequence the user has control of how they want to use the program as an example they have the ability to choose how to initialize centroids, either by k-random centroids, first k-points or by the described kmeans++ algorithm.\\
In the following chapter I will desribe and detail some of the ways I implemented the algorithm and a chronological order and the issues and solutions i combated.
\section{Try: Python}
\label{subsec:beginning}
To start of with the goal was to implement a barebones version of the kmeans algorithm in python without use of any external libraries, this decision means that we will have no array programming or vectorization. So we are left with only our knowledge of programming and a blank editor.
This means that we are forced to use loops in python, luckily python includes the feature: list comprehension which is a way of creating new list from  old list depending on some condition. This can get rid of some larger loops and as a bonus list comprehension is optimized for the python interpreter, just take a look this example where we choose k centroids from the set of points, first with a loop the standard way where we have to initialize the list to store the centroids in first, and then with a list comprehension.

\begin{lstlisting}[language=Python]
  centroids = []
  for i in range(k):
      centroids.append(points[randint(0, len(data)-1)])
\end{lstlisting}
Now with list comprehension
\begin{lstlisting}[language=Python]
centroids = [points[randint(0, len(data)-1)] for i in range(k)]
\end{lstlisting}
One way or the other we still loop sequentially k times and doing it this way is not a vectorized approach thus we cannot hope to accelerate it in any meaningful way. A vectorized version will be discussed in the next chapter where start discussing the basic implementation using Numpy. \\
Another maybe even more surprising example, is the assignment step of Lloyd's algorithm where we have to compute the distance from each point to each centroid, without any vectorization we are again forced to use for loops, but this time a nested one

\begin{lstlisting}[language=Python]
  for p_idx, point in enumerate(points):
      distance_array = [0] * k #Initialize empty array
      for c_idx, centroids in enumerate(centroids):
          distance = euclidean_distance(point, centroid)
          distance_array[c_idx] = distance
      labels[p_idx] = distance_array.index(min(distance_array))
\end{lstlisting}

This is definitely hurtful for performance as we have to execute the body of the loop $n \times k$ times (not to mention the loop in the euclidean distance function)



\newpage
\section{Except: Numpy}
\label{subsec:middle}
Now we turn our heads to a version written with the help of the library NumPy, NumPy allows us to utilize array programming as already discussed. This makes it so that we can carry out array operations with simple instructions and then NumPy will take the instructions and carry them out in a much more optimized way. Let us take a quick look at the previous code snippet where we chose $k$ random centroids, but this time using NumPy

\begin{lstlisting}[language=Python]
import numpy as np
row_i = np.random.choice(points.shape[0], k)
centroids=points[row_i,:]
\end{lstlisting}

This let's NumPy handle all the dirty work and should in theory give better performance, lets just take a quick look at what speedups we might expect. We performed a test with the previous code snippets discussed for choosing k random centroids from a set of points. The results can be seen in figure \ref{fig:numpyvspython}, the speedups from choosing 5000 centroids is about 13 times faster when going from Python to NumPy, we could expect to see even greater results when going to larger values of $k$.

\begin{figure} [H]
  \centering
  \includegraphics[width=0.9\linewidth]{images/pybnumpy.pdf}
  \caption{Benchmark of choosing k random centroids from a set of points, first using list comprehension in python, then with NumPy. The number of centroids chosen is \{10, 20, 50, 100, 1000, 5000\}}
  \label{fig:numpyvspython}
\end{figure}

So in order to minimize time spent on computation we should utilize NumPy's array programming whenever possible, this means in some cases rewriting code that seemingly seems fast and correct just to push some small gains in performance.
As discussed in section \ref{subsec:lloyds} Lloyd's algorithm will for each iteration need $3d \cdot n \cdot k$ operations to calculate the Euclidean distance between two points, so this will indeed a focus point to vectorize, where we are also able to make us of some of NumPy's more advanced features such as broadcasting to implement a faster vectorized version, lets take a look at an example in algorithm \ref{alg:eucdist}. As we see there are now no for loops to deal with and once the special features of NumPy is grasped the notation is also much simpler.

% \begin{algorithm}
%   \caption{Euclidâ€™s algorithm}
%   \label{euclid}
%   \begin{algorithmic}[1]
%     \Procedure{Euclid}{$a,b$}
%     \Comment{The g.c.d. of a and b}
%     \State $r\gets a\bmod b$
%     \While{$r\not=0$}
%     \Comment{We have the answer if r is 0}
%     \State $a\gets b$
%     \State $b\gets r$
%     \State $r\gets a\bmod b$
%     \EndWhile
%     \label{euclidendwhile}
%     \State \textbf{return} $b$
%     \Comment{The gcd is b}
%     \EndProcedure
%   \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
  \caption{Euclidean Distance Numpy}
  \label{alg:eucdist}
  \begin{algorithmic}[1]
    \Procedure{$EUC\_DISTANCE$}{Points, centroids}
    \State $X \gets $ points - centroids$[:,None]$
    \Comment{Numpy Broadcasting}
    \State Distances $\gets np.sqrt(X * X).sum(2)$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

In the second step of the kmeans algorithm we have to move the centroids, using a seemingly naive approach this would take two for loops taking around a running time of $k^2$ times to complete. This can be seen in algorithm \ref{alg:movecent}, and then a fast vectorized approach again using some advanced NumPy features to rid ourselves of loops can be seen in code snippet \ref{code:movecent}, our gain in speeds when going from the sequential to vectorized approach when be discussed in more detail in the results section.


\begin{algorithm}
  \caption{Move Centroids}
  \label{alg:movecent}
  \begin{algorithmic}[1]
    \Procedure{$move\_centroids$}{}
    \For{$i \gets 0, k$}
    \ForAll{points belonging to centroid $i$}
    \State $sum \gets sum + points$
    \EndFor
    \State centroid[i] = sum / len(sum)
    \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}



\begin{python}[caption={Numpy Vectorized version of move centroids}\label{code:movecent}]
import numpy as np
def move_centroids(points, closest, centroids, k):
    mask = (labels == np.arange(k)[:,None])
    return mask.dot(points)/ mask.sum(1)[:,None]
\end{python}

\newpage

\section{Finally: Bohrium}
\label{subsec:finally}
Let us now turn our heads to the final beast that is Bohrium, automated acceleration of our matrix operations sounds awesome, just replace the import of NumPy, with an import of Bohrium in our previous code and all the code should work, super fast kmeans right? Not so fast, not all NumPy function's have been implemented yet, and some are just not able to be accelerated, then we have two choices going forward, either find another way to write the code such that Bohrium can automatically accelerate it, or write a custom userkernel for you problem. In my efforts to create a version using Bohrium, I have resorted to both methods, sometimes going back and forth when my own knowledge of the problem expanded, thus rewriting and refactoring the same code multiple times getting a better and better implementation.


\chapter{Results}
\label{sec:label}




\chapter{Experiments}
\label{sec:experi}

\chapter{Conclusion}
\label{sec:label}



\newpage

\bibliographystyle{plain}
\bibliography{ref,main}

\end{document}
